{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839d730d",
   "metadata": {},
   "source": [
    "# Workshop - MELI Data Challenge 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60e76a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from itertools import chain, islice\n",
    "from datetime import timedelta\n",
    "import jsonlines\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import core.evaluators.metrics as metrics\n",
    "import multiprocessing as mp\n",
    "from itertools import chain, islice\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64956bc3",
   "metadata": {},
   "source": [
    "### 1. Fetching the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f9b4da",
   "metadata": {},
   "source": [
    "#### Load train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46650563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the directory where the challenge data is stored\n",
    "data_dir = Path('challenge_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc92e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_parquet(data_dir/'train_data.parquet')\n",
    "data_test = pd.read_csv(data_dir/'test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67b0f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609cc721",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a772f7",
   "metadata": {},
   "source": [
    "#### Load extra item data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd7284",
   "metadata": {},
   "outputs": [],
   "source": [
    "### auxiliary function to read jsonlines files\n",
    "def load_jsonlines(filename):\n",
    "    \n",
    "    rv = []\n",
    "    for obj in tqdm(jsonlines.open(filename)):\n",
    "        rv.append(obj)\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4c5b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_metadata = load_jsonlines(data_dir/'items_static_metadata_full.jl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708fa21f",
   "metadata": {},
   "source": [
    "#### Convert to a df and use sku as the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e714ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata = pd.DataFrame(item_metadata)\n",
    "df_metadata.index = df_metadata.sku\n",
    "df_metadata.drop(columns=['sku'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d61feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82752021",
   "metadata": {},
   "source": [
    "#### Hydrate the initial datasets with the extra data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.join(df_metadata, on='sku',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704fa798",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test.join(df_metadata, on='sku',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149df073",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b9112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966fce6b",
   "metadata": {},
   "source": [
    "### 2. Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e307c37",
   "metadata": {},
   "source": [
    "#### List all the columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fbf59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data_train.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9102a9",
   "metadata": {},
   "source": [
    "#### Get some stats for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceefc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcee9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_cols(cols,df):\n",
    "    \n",
    "    for col in cols: \n",
    "        print('\\t COLUMN: ', col)\n",
    "        print('\\t type: ', df[col].dtype,'\\n')\n",
    "        print(df[col].describe(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98efdbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_describe = ['date','listing_type','current_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc364af",
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_cols(columns_to_describe,data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46d35f2",
   "metadata": {},
   "source": [
    "### Visualize the time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7329d4",
   "metadata": {},
   "source": [
    "#### Visualize daily sales grouped by site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bd0204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we summarize the info\n",
    "summary_site = data_train.groupby(['site_id','date']).sold_quantity.sum().reset_index()\n",
    "summary_site.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b72d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(summary_data,time_var,series,level):\n",
    "    \n",
    "    plt.figure(figsize=(15, 4))\n",
    "    plt.title(f'{series} time series grouped by {level}')\n",
    "    sns.lineplot(data=summary_data, \n",
    "                 x=time_var,y=series,hue=level)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee64df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we plot it\n",
    "plot_time_series(summary_site, time_var='date',series='sold_quantity',level='site_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5d3682",
   "metadata": {},
   "source": [
    "#### Visualize weekly sales grouped by site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new variable based on the date column to extract the week number\n",
    "data_train['week'] = pd.to_datetime(data_train.date).dt.week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947a5f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize info\n",
    "summary_site_w = data_train.groupby(['site_id','week']).sold_quantity.sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e89c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we plot it\n",
    "plot_time_series(summary_site_w,time_var='week',series='sold_quantity',level='site_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3605302",
   "metadata": {},
   "source": [
    "#### Get the top levels of categorical variable for a site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acb1746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_categories(df, categorical_var, site_id, by, N=10):\n",
    "    \n",
    "    grand_total = df[df.site_id == site_id][by].sum()\n",
    "  \n",
    "    top_cat_df = (df[df.site_id == site_id]\n",
    "         .groupby(['site_id',categorical_var])[by]\n",
    "         .sum()\n",
    "         .sort_values(ascending=False)\n",
    "         .head(N))\n",
    "    \n",
    "    top_cat_df = top_cat_df.reset_index()\n",
    "    top_cat_df[f'relative_{by}'] = top_cat_df[by]/grand_total \n",
    "    \n",
    "    return(top_cat_df[[categorical_var,by,f'relative_{by}']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070babd",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_domains_MLM = get_top_categories(data_train, \n",
    "                                     categorical_var= 'item_domain_id',\n",
    "                                     site_id='MLM', \n",
    "                                     by='sold_quantity', \n",
    "                                     N=10)\n",
    "top_domains_MLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3910c715",
   "metadata": {},
   "source": [
    "#### Asses overlap between train and test skus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b923f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed984597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asses_overlap(df_train, df_test, key):\n",
    "    \n",
    "    figure, axes = plt.subplots(1, len(df_train.site_id.unique()),figsize=(16, 6))\n",
    "\n",
    "    for i,site in enumerate(df_train.site_id.unique()):\n",
    "\n",
    "        unique_train = df_train[df_train.site_id == site][key].unique()\n",
    "        unique_test = df_test[df_test.site_id == site][key].unique()\n",
    "\n",
    "        v = venn2(subsets=[set(unique_train),set(unique_test)],\n",
    "                  set_labels = (f\"Train \\n ({len(unique_train)})\", \n",
    "                        f\"Test \\n ({len(unique_test)}) \"),\n",
    "                  ax=axes[i],\n",
    "                  set_colors=('purple', 'skyblue'), alpha = 0.6)\n",
    "        axes[i].set_title(site)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e3472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "asses_overlap(data_train, data_test, key='sku')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979ff51",
   "metadata": {},
   "source": [
    "#### Plot distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26dedba",
   "metadata": {},
   "source": [
    "##### Plot distribution for continuos variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbcf12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_id = 'MLM'\n",
    "item_domain_id = 'MLM-CELLPHONE_COVERS'\n",
    "#product_id = 'MLM15586828'\n",
    "subset_data = data_train[(data_train.site_id == site_id)& (data_train.item_domain_id == item_domain_id)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bef4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data.current_price.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660e779",
   "metadata": {},
   "source": [
    "##### Plot distribution for categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8589df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data.shipping_logistic_type.value_counts(normalize=True).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfbdeef",
   "metadata": {},
   "source": [
    "#### Plot the relationship between two continuos variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_id = 'MLM'\n",
    "item_domain_id = 'MLM-CELLPHONE_COVERS'\n",
    "subset_data = data_train[(data_train.site_id == site_id)& (data_train.item_domain_id == item_domain_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f516b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bivariate(data,level, x, y, agg_x, agg_y):\n",
    "    \n",
    "    sns.scatterplot(data=data.groupby(level).agg(\n",
    "        {x: agg_x,y: agg_y}),\n",
    "                    x=x,y=y)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eaa6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bivariate(subset_data,\n",
    "               x='current_price',\n",
    "               level='sku',\n",
    "               y='sold_quantity', \n",
    "               agg_x=np.mean, \n",
    "               agg_y=np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f46a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bivariate(subset_data,\n",
    "               level='sku',\n",
    "               x='minutes_active',\n",
    "               y='sold_quantity', \n",
    "               agg_x=np.mean, \n",
    "               agg_y=np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e830cdff",
   "metadata": {},
   "source": [
    "#### Distribution of target stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d01fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(1, 2,figsize=(14, 6))\n",
    "figure.suptitle('Distribution of target stock')\n",
    "sns.histplot(x=data_test.target_stock,bins=5000, kde=False, ax=axes[0])\n",
    "axes[0].set_xlim(0,80)\n",
    "sns.boxplot(x=data_test.target_stock, ax=axes[1])\n",
    "axes[1].set_xlim(0,80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf092b21",
   "metadata": {},
   "source": [
    "### 3. Building your validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc18e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.date.min(), data_train.date.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254f708b",
   "metadata": {},
   "source": [
    "##### Make a temporary split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34917948",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = (pd.to_datetime(data_train.date).max()-timedelta(days=30)).date()\n",
    "print(split_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a16ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separete the last 30 days for validation\n",
    "data_val = data_train.loc[(data_train.date > str(split_date))]\n",
    "\n",
    "#use the rest as training\n",
    "data_train = data_train.loc[(data_train.date <= str(split_date))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8f399",
   "metadata": {},
   "source": [
    "##### Now let's build the validation dataset by calculating target stock and inventory days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7fdbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#disclaimer: this is not the code that was used to generate the test_set.\n",
    "# It was made from scratch\n",
    "\n",
    "def create_validation_set(dataset):\n",
    "    np.random.seed(42)\n",
    "    print('Sorting records...')\n",
    "    temp_pd = dataset.loc[:, ['sku','date','sold_quantity']].sort_values(['sku','date'])\n",
    "\n",
    "    print('Grouping quantity...')\n",
    "    temp_dict = temp_pd.groupby('sku').agg({'sold_quantity':lambda x: [i for i in x]})['sold_quantity'].to_dict()\n",
    "\n",
    "    result = []\n",
    "    for idx, list_quantity in tqdm(temp_dict.items(), desc='Making targets...'):\n",
    "        cumsum = np.array(list_quantity).cumsum()\n",
    "        stock_target = 0\n",
    "        if cumsum[-1] > 0 and len(cumsum)==30:\n",
    "            \n",
    "            #choose a random target different from 0\n",
    "            while stock_target == 0:\n",
    "                stock_target = np.random.choice(cumsum)\n",
    "                \n",
    "            #get the first day with this amounnt of sales\n",
    "            day_to_stockout = np.argwhere(cumsum==stock_target).min() + 1\n",
    "            \n",
    "            #add to a list\n",
    "            result.append({'sku':idx, 'target_stock':stock_target, 'inventory_days':day_to_stockout})\n",
    "    return result\n",
    "\n",
    "#generate target for the 30 days of validation\n",
    "val_dataset = create_validation_set(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3601d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ccf4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_val = [x['inventory_days'] for x in val_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0ff699",
   "metadata": {},
   "source": [
    "### 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ae394c",
   "metadata": {},
   "source": [
    "#### Baseline #1: UNIFORM distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9950a5c8",
   "metadata": {},
   "source": [
    "We need a baseline to know what is our starting point. We will use it latter to validate more complex models.  \n",
    "Besides we could iterate a simple baseline model to get better models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769cf21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_to_predict = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e26a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_uniform = [(np.ones(days_to_predict)/days_to_predict).round(5).tolist()] * len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0e8b8d",
   "metadata": {},
   "source": [
    "This is how a uniform distribution baseline output would look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d831612",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_pred_uniform, columns=range(1,days_to_predict+1)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c686b4",
   "metadata": {},
   "source": [
    "##### How the inventory_days probability distribution looks like for a random observation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66049dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sku, stock,  days = pd.DataFrame(val_dataset)[['sku','target_stock','inventory_days']].sample(1).to_dict(orient='records')[0].values()\n",
    "plt.ylim([0,0.05])\n",
    "plt.axvline(days, color='r')\n",
    "plt.title(f'sku:{sku}, target_stock:{stock},target days: {days}')\n",
    "plt.bar(range(1,31), np.ones(days_to_predict)/days_to_predict, color='green')\n",
    "\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Probs')\n",
    "plt.legend(['Target days', 'Uniform Dist.'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda32e7a",
   "metadata": {},
   "source": [
    "##### Now let's score this model's prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3503a1",
   "metadata": {},
   "source": [
    "##### Scoring function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70fc276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranked_probability_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        y_true: np.array of shape 30. \n",
    "        y_pred: np.array of shape 30. \n",
    "    \"\"\"\n",
    "    return ((y_true.cumsum(axis=1) - y_pred.cumsum(axis=1))**2).sum(axis=1).mean()\n",
    "\n",
    "def scoring_function(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        y_true: List of Ints of shape Nx1. Contain the target_stock\n",
    "        y_pred: List of float of shape Nx30. Contain the prob for each day\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true_one_hot = np.zeros_like(y_pred, dtype=np.float)\n",
    "    y_true_one_hot[range(len(y_true)), y_true-1] = 1\n",
    "    return ranked_probability_score(y_true_one_hot, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f86c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_score = scoring_function(y_true_val, y_pred_uniform)\n",
    "print('Uniform model got a validation RPS of: ',uniform_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b867248",
   "metadata": {},
   "source": [
    "***In the public leaderboard this approach got a score of 5.07***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484ee1dc",
   "metadata": {},
   "source": [
    "#### Baseline #2: Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bacbe4e",
   "metadata": {},
   "source": [
    "As the uniform distributioin works so well, the idea is to slighly move the distribution toward the target day.\n",
    "To do so we are going to use a very wide normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421cad0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_batch_predictions(model, x_test, batch_size=10000, processors=20):\n",
    "    \"\"\"Function usefull for paralellize inference\"\"\"\n",
    "    pool = mp.Pool(processors)\n",
    "    batches = batchify(x_test,batch_size)\n",
    "    results = pool.imap(model.predict_batch,batches)\n",
    "    pool.close()\n",
    "    output = []\n",
    "    for r in tqdm(results, total=int(len(x_test)/batch_size), desc='generating preds...'):\n",
    "        output.extend(r)\n",
    "    preds_dict = {}\n",
    "    for sku,probs in tqdm(output):\n",
    "        preds_dict[sku] = probs\n",
    "    y_pred = []\n",
    "    for x in tqdm(x_test):\n",
    "        pred = preds_dict[x['sku']]\n",
    "        y_pred.append(pred)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def batchify(iterable, batch_size):\n",
    "    \"\"\"Convert an iterable in a batch-iterable\"\"\"\n",
    "    iterator = iter(iterable)\n",
    "    for first in iterator:\n",
    "        yield list(chain([first], islice(iterator, batch_size - 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34835ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "step=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4953549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = norm(15, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a460f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if step >= 1:\n",
    "    x_axis = np.arange(-10, 40, 0.001)\n",
    "    plt.plot(x_axis, model_.pdf(x_axis))\n",
    "    plt.legend(['Normal dist'])\n",
    "\n",
    "if step >= 2:    \n",
    "    plt.axvline(0, color='black')\n",
    "    plt.axvline(30, color='black')\n",
    "    \n",
    "if step >= 3:\n",
    "    for i in range(30):\n",
    "        plt.vlines(i,ymin=0,ymax=model_.pdf(i))\n",
    "\n",
    "if step >= 4:\n",
    "    scale = model_.cdf(30) - model_.cdf(0)\n",
    "    x_axis = np.arange(0, 31, 1)\n",
    "    plt.plot(x_axis, model_.pdf(x_axis)/scale)\n",
    "    step = 0\n",
    "step += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eeca8d",
   "metadata": {},
   "source": [
    "##### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29094c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "\n",
    "class LinearModel():\n",
    "    \"\"\"\n",
    "    Linear model based on sold_quantity\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 last_n_days=None, \n",
    "                 normalize=True):\n",
    "        \n",
    "        self.normalize = normalize\n",
    "        self.last_n_days = last_n_days\n",
    "        self.border_cases = 0\n",
    "        self.normal_cases = 0\n",
    "        \n",
    "    def fit(self, data):\n",
    "        \"\"\" Store mean and std-dev for each SKU \"\"\"\n",
    "        \n",
    "        if self.last_n_days != None:\n",
    "            min_training_date = str((pd.to_datetime(data.date.max())-timedelta(days=self.last_n_days)).date())\n",
    "        else:\n",
    "            min_training_date = str(data.date.min().date())\n",
    "            \n",
    "        self.parameters = (data[data.date >= min_training_date]\n",
    "                           .groupby('sku')\n",
    "                           .agg({'sold_quantity':['mean', 'std']})\n",
    "                           .sold_quantity\n",
    "                           .to_dict())\n",
    "\n",
    "        self.general_mean = data.sold_quantity.mean()\n",
    "        self.general_std = data.sold_quantity.std()\n",
    "        return self \n",
    "    \n",
    "    def calc_probs(self, norm_dist):\n",
    "        #cut probs in days\n",
    "        probs = []\n",
    "        for i in range(1, 31):\n",
    "            probs.append(norm_dist.cdf(i+1) - norm_dist.cdf(i))\n",
    "        \n",
    "        #if prob is zero, replace with uniform\n",
    "        if np.sum(probs) == 0:\n",
    "            return np.ones(30) / 30\n",
    "\n",
    "        if self.normalize:\n",
    "            probs = probs / np.sum(probs)\n",
    "        return probs\n",
    "    \n",
    "    def predict(self, idx, stock):\n",
    "        \"\"\" calculate mean and variance to stockout for a given SKU \"\"\"\n",
    "        #retrieve the mean and variance for the SKU\n",
    "        if self.parameters['mean'].get(idx, 0.) != 0.:\n",
    "            mean = self.parameters['mean'][idx]\n",
    "            std = self.parameters['std'][idx]        \n",
    "            self.normal_cases += 1\n",
    "        else:\n",
    "            #to catch border cases where there is no data in train or has all 0s.\n",
    "            mean = self.general_mean\n",
    "            std = self.general_std    \n",
    "            self.border_cases += 1\n",
    "            \n",
    "        if std == 0. or np.isnan(std):\n",
    "            std = self.general_std\n",
    "        \n",
    "        #convert quantities into days\n",
    "        days_to_stockout = stock / mean\n",
    "        std_days = (std / mean) * days_to_stockout\n",
    "        return days_to_stockout, std_days\n",
    "    \n",
    "    def predict_proba(self, idx, stock):\n",
    "        \"\"\" Calculates the 30 days probs given a SKU and a target_stock \"\"\"\n",
    "        days_to_stockout, std_days = self.predict(idx, stock)\n",
    "        norm_dist = norm(days_to_stockout, std_days)\n",
    "        return self.calc_probs(norm_dist)\n",
    "    \n",
    "    def predict_batch(self, X, proba=True):\n",
    "        \"\"\" \n",
    "        Predict probs for many SKUs \n",
    "        Input:\n",
    "            X: List of Dicts with keys sku and target_stock\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for x in X:\n",
    "            idx = x['sku']\n",
    "            stock = x['target_stock']\n",
    "            if proba:\n",
    "                result.append((idx, self.predict_proba(idx, stock)))\n",
    "            else:\n",
    "                result.append((idx, self.predict(idx, stock)))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c100b1f8",
   "metadata": {},
   "source": [
    "##### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737867bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = LinearModel(last_n_days=14, normalize=True)\n",
    "\n",
    "#train the model with train data\n",
    "model.fit(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e162ae6",
   "metadata": {},
   "source": [
    "##### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39ec394",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_normal = generate_batch_predictions(model, val_dataset, batch_size=10000, processors=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4bd945",
   "metadata": {},
   "source": [
    "##### How the inventory_days probability distribution looks like for a random observation in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb7a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "\n",
    "sku, stock,  days = pd.DataFrame(val_dataset)[['sku','target_stock','inventory_days']].sample(1).to_dict(orient='records')[0].values()\n",
    "probs = model.predict_proba(sku, stock)\n",
    "mean_to_stockout, var_to_stockout = model.predict(sku, stock)\n",
    "plt.bar(range(1,31), probs)\n",
    "plt.axvline(days, color='r')\n",
    "plt.title('sku:{}, target_stock:{}, mean: {}, std:{}'.format(int(sku), \n",
    "                                                             stock,\n",
    "                                                             round(mean_to_stockout), \n",
    "                                                             round(var_to_stockout)))\n",
    "plt.axhline(1/30, color='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4acd50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the score\n",
    "normal_score = scoring_function(y_true_val, y_pred_normal)\n",
    "print('Normal distribution model got a validation RPS of: ',normal_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b92a45",
   "metadata": {},
   "source": [
    "### 5. Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb22546",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_pd = pd.DataFrame(val_dataset)\n",
    "scores = []\n",
    "for y_t, y_p in tqdm(zip(val_dataset_pd['inventory_days'].to_list(), y_pred_normal)):\n",
    "    scores.append(scoring_function(np.array([int(y_t)]), np.array([y_p])))\n",
    "val_dataset_pd.loc[:, 'score'] = scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecdc6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(val_dataset_pd.iloc[:10000].inventory_days, val_dataset_pd.iloc[:10000].score)\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Score by days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f403609",
   "metadata": {},
   "source": [
    "Here we see ...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5b31d6",
   "metadata": {},
   "source": [
    "### 6. Train model to submit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdbcadf",
   "metadata": {},
   "source": [
    "Now that we have validated that the approach works, we train the model with all the data in order to make a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db692fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([data_train,data_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b546d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(last_n_days=14, normalize=True)\n",
    "\n",
    "model.fit(all_data) #   <---- HERE WE TRAIN THE MODEL WITH FULL DATA !!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5559ae84",
   "metadata": {},
   "source": [
    "##### Generate predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2961434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = data_test.reset_index()[['index','sku','target_stock']].to_dict(orient='records')\n",
    "\n",
    "y_pred = generate_batch_predictions(model, x_test, batch_size=10000, processors=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f1b8a",
   "metadata": {},
   "source": [
    "##### Finally we generate a submission file with the model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e5352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array2text(y_pred):\n",
    "    \"\"\"convert a list of number in a list of texts with 4 decimal positions \"\"\"\n",
    "    result = []\n",
    "    for xs in tqdm(y_pred):\n",
    "        line = []\n",
    "        for x in xs:\n",
    "            line.append('{:.4f}'.format(x))\n",
    "        result.append(line)\n",
    "    return result\n",
    "\n",
    "def make_submission_file(y_pred, file_name='submission_file', compress=True, single_row=True):\n",
    "    \"\"\"Convert a list of text into a submition file\"\"\"\n",
    "    result = array2text(y_pred)\n",
    "    if compress:\n",
    "        if single_row:\n",
    "            file_name = f'{file_name}.csv.gz'\n",
    "            with gzip.open(file_name, \"wt\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                for row in tqdm(result, desc='making file...'):\n",
    "                    writer.writerow(row)\n",
    "        else:\n",
    "            file_name = f'{file_name}.csv.gz'\n",
    "            with gzip.open(file_name, \"wt\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerows(result)\n",
    "    else:\n",
    "        if single_row:\n",
    "            file_name = f'{file_name}.csv'            \n",
    "            with open(file_name, \"w\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                for row in tqdm(result, desc='making file...'):\n",
    "                    writer.writerow(row)\n",
    "        else:\n",
    "            file_name = f'{file_name}.csv'\n",
    "            with open(file_name, \"w\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerows(result)\n",
    "    return file_name\n",
    "\n",
    "def read_submission_file(file_name, compress=False):\n",
    "    if compress:\n",
    "        with gzip.open(file_name, 'rt') as f:\n",
    "            submission = f.read()\n",
    "    else:\n",
    "        with open(file_name, 'r') as f:\n",
    "            submission = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b3711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = make_submission_file(y_pred, 'submittion_file_linear_model', compress=True, single_row=True)\n",
    "print(f'Submission file created at: {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b01a78b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
